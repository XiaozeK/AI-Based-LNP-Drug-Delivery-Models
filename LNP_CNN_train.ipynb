{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for t=1 to t=20\n",
    "Taking first three convolutional and pooling layers (first three blocks) of VGG16. In the final block extract the final convolutional layer and add on top of the network the following layers:\n",
    "1. Batch normalization\n",
    "2. Global average pooling\n",
    "3. Latent vector\n",
    "4. Batch normalization (size 32 - this is the feature vector for use in the time-series modelling)\n",
    "5. RELU activation\n",
    "6. Final dense layer of size 1 (with a sigmoid activation for classification mode and no activation when in prediction/regression mode).\n",
    "\n",
    "\n",
    "### Modelling info\n",
    "5 fold cross-validation on training data. Binary cross-entropy loss for classification and MSE loss for the regressor along with the Adam optimizer (with a learning rate of 0.001). Batch size of 32. Training for 50 epochs with the base model frozen. 50 epochs was sufficient for convergence. Unfreezing the base after and training for more epochs did not lower the validation loss, so here the base is kept frozen. 90 degree rotations and mirroring were used to augment the training data to 8 times its size. Augmentation was also used on the validation data (and will also be used in the end, in later notebooks, on the test data). Save best weights as go along and best model at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import re\n",
    "import glob\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Activation, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model_history, model_name, fit_method):\n",
    "    fig = plt.figure(figsize=(8,5), facecolor='w')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(model_history.history['loss'])\n",
    "    ax.plot(model_history.history['val_loss'])\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path, stats, rescale_method):\n",
    "    img = np.load(path)\n",
    "    img = img.astype('float32')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gfp(path, stats, rescale_method, fit_method, gfp_thresh):\n",
    "    gfp = np.load(path)\n",
    "    if fit_method == 'classify':\n",
    "        if gfp <= gfp_thresh:\n",
    "            res = 0.\n",
    "        else:\n",
    "            res = 1.\n",
    "    if fit_method == 'regress':\n",
    "        if rescale_method == 'log':\n",
    "            res = np.log(gfp)\n",
    "        if rescale_method == 'log_center':\n",
    "            res = np.log(gfp) - np.log(stats[2])\n",
    "        if rescale_method == 'normalize':\n",
    "            res = (gfp - stats[0])/(stats[1] - stats[0])\n",
    "        if rescale_method == 'standardize':\n",
    "            res = (gfp - stats[2])/stats[3]\n",
    "        if rescale_method == 'log_normalize':\n",
    "            res = (np.log(gfp) - np.log(stats[0]))/(np.log(stats[1]) - np.log(stats[0]))\n",
    "        if rescale_method == 'log_standardize':\n",
    "            res = (np.log(gfp) - np.log(stats[2]))/np.log(stats[3])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate90xDegrees(img):\n",
    "    ret =[]\n",
    "    rows, cols = img.shape\n",
    "    for i in range(1,4):\n",
    "        M = cv2.getRotationMatrix2D((cols/2,rows/2),i*90,1)\n",
    "        ret.append(cv2.warpAffine(img,M,(cols,rows)))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img, aug_no):\n",
    "    img_new = np.zeros((img.shape)).astype('float32')\n",
    "    rows, cols, chans = img.shape\n",
    "    for i in range(chans):\n",
    "        if aug_no == 0:\n",
    "            img_new[:, :, i] = img[:, :, i]\n",
    "        if aug_no == 1:\n",
    "            img_new[:, :, i] = np.flipud(img[:, :, i])\n",
    "        if aug_no == 2:\n",
    "            img_new[:, :, i] = np.fliplr(img[:, :, i])\n",
    "        if aug_no == 3:\n",
    "            ud = np.flipud(img[:, :, i])\n",
    "            img_new[:, :, i] = np.fliplr(ud)\n",
    "        if aug_no == 4:\n",
    "            img_new[:, :, i] = rotate90xDegrees(img[:, :, i])[0]\n",
    "        if aug_no == 5:\n",
    "            ud = np.flipud(img[:, :, i])\n",
    "            img_new[:, :, i] = rotate90xDegrees(ud)[0]\n",
    "        if aug_no == 6:\n",
    "            lr = np.fliplr(img[:, :, i])\n",
    "            img_new[:, :, i] = rotate90xDegrees(lr)[0]\n",
    "        if aug_no == 7:\n",
    "            ud = np.flipud(img[:, :, i])\n",
    "            udlr = np.fliplr(ud)\n",
    "            img_new[:, :, i] = rotate90xDegrees(udlr)[0]\n",
    "    \n",
    "    return img_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(cell_files, gfp_files, batch_size, cell_stats, gfp_stats, \n",
    "                          cell_rescale_method, gfp_rescale_method, fit_method):\n",
    "    while True:\n",
    "        # Select files (paths/indices) for the batch\n",
    "        samp = np.random.choice(np.arange(0, len(cell_files)), size=batch_size, replace=False)\n",
    "        batch_paths_in =  [cell_files[index] for index in samp]\n",
    "        batch_paths_out = [gfp_files[index] for index in samp]\n",
    "        batch_input = []\n",
    "        batch_output = []\n",
    "        aug_nums = np.random.randint(0, 8, batch_size)\n",
    "        \n",
    "        for ii, image_path in enumerate(batch_paths_in):\n",
    "            img = get_image(image_path, stats=cell_stats, rescale_method=cell_rescale_method)\n",
    "            aug_no = aug_nums[ii]\n",
    "            img = augment_image(img, aug_no=aug_no)\n",
    "            batch_input += [img]\n",
    "        \n",
    "        for ii, path in enumerate(batch_paths_out):\n",
    "            target = get_gfp(path, stats=gfp_stats, rescale_method=gfp_rescale_method, \n",
    "                             fit_method=fit_method, gfp_thresh=gfp_stats[4])\n",
    "            batch_output += [target]\n",
    "        \n",
    "        batch_x = np.array(batch_input)\n",
    "        batch_y = np.array(batch_output)\n",
    "        \n",
    "        if fit_method == 'classify':\n",
    "            batch_y = batch_y.astype('int64')\n",
    "        \n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and modelling settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 'fold5' # this is the validation fold, all cells not in this fold used for proper training\n",
    "unfreeze_base = 'False'\n",
    "\n",
    "all_cell_dir = '/scratch-shared/phil/LNP/LNP_data_09/images_train'\n",
    "all_cell = glob.glob(all_cell_dir + '/' '*')\n",
    "all_cell = sorted(all_cell, key = natural_keys)\n",
    "all_gfp_dir = '/scratch-shared/phil/LNP/LNP_data_09/gfp_train'\n",
    "\n",
    "train_cell = []\n",
    "train_gfp = []\n",
    "\n",
    "valid_cell = []\n",
    "valid_gfp = []\n",
    "\n",
    "for cell in all_cell:\n",
    "    s0 = cell.split('train/')\n",
    "    s1 = s0[1].split('_')\n",
    "    if s1[0] == fold:\n",
    "        valid_cell.append(cell)\n",
    "        f = glob.glob(all_gfp_dir + '/' + s1[1] + '_' + s1[2] + '_T0072_cell_' + s1[5])\n",
    "        valid_gfp.extend(f)\n",
    "    else:\n",
    "        train_cell.append(cell)\n",
    "        f = glob.glob(all_gfp_dir + '/' + s1[1] + '_' + s1[2] + '_T0072_cell_' + s1[5])\n",
    "        train_gfp.extend(f)\n",
    "\n",
    "\n",
    "fit_method = 'regress' # 'classify' or 'regress'\n",
    "# previous explorations showed the following two rescaling methods to be the most appropriate\n",
    "cell_rescale_method = 'standardize'\n",
    "gfp_rescale_method = 'log_center'\n",
    "cell_stats = np.load('/scratch-shared/phil/LNP/LNP_data_09/cell_stats.npy')\n",
    "gfp_stats = np.load('/scratch-shared/phil/LNP/LNP_data_09/gfp_stats.npy')\n",
    "\n",
    "# getting class weights for when in classification mode\n",
    "if fit_method == 'classify':\n",
    "    class_gfp = []\n",
    "    for cell in train_gfp:\n",
    "        gfp = get_gfp(cell, stats=gfp_stats, rescale_method=gfp_rescale_method, \n",
    "                      fit_method=fit_method, gfp_thresh=gfp_stats[4])\n",
    "        class_gfp = np.append(class_gfp, gfp)\n",
    "    class_gfp = class_gfp.astype('int64')\n",
    "    weights = class_weight.compute_class_weight('balanced', np.unique(class_gfp), class_gfp)\n",
    "    print('weights = ' + str(weights))\n",
    "\n",
    "save_best = keras.callbacks.ModelCheckpoint('best.weights', monitor='val_loss', verbose=0, save_best_only=True)\n",
    "batch_size = 32\n",
    "latent_dim = 32\n",
    "num_epochs = 50\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 192, 192, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 192, 192, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 192, 192, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 96, 96, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 96, 96, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 96, 96, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 48, 48, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 48, 48, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 48, 48, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 48, 48, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 256)       1024      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "latent_vector (BatchNormaliz (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,744,897\n",
      "Trainable params: 1,744,321\n",
      "Non-trainable params: 576\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = VGG16(weights='imagenet',\n",
    "                         include_top=False,\n",
    "                         input_shape=(192, 192, 3))\n",
    "\n",
    "base_model = Model(inputs=pretrained_model.input, outputs=pretrained_model.get_layer('block3_conv3').output)\n",
    "x = base_model.output\n",
    "x = BatchNormalization()(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(latent_dim)(x)\n",
    "x = BatchNormalization(name='latent_vector')(x)\n",
    "x = Activation('relu')(x)\n",
    "if fit_method == 'classify':\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "else:\n",
    "    predictions = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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